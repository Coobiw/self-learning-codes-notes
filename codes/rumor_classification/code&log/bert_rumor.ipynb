{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charon/anaconda3/envs/qbw_base/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f466cf927d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "#PyTorch用的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 自然语言处理相关的包\n",
    "import re #正则表达式的包\n",
    "import jieba #结巴分词包\n",
    "from collections import Counter #搜集器，可以让统计词频更简单\n",
    "\n",
    "#绘图、计算用的程序包\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# 设置随机种子保证可复现\n",
    "import random\n",
    "SEED = 729608\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# os python hash seed, make experiment reproducable\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "# gpu algorithom \n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Generator SEED\n",
    "Generator = torch.Generator()\n",
    "Generator.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读取数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3389/3389 [00:01<00:00, 2185.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结束, 有1538条谣言, 有1849条非谣言!\n",
      "['＂有时最坚强的人是 - 不因缺陷而不爱，关起门才哭泣，和打没有人知道的仗。＂(Sometimes the strongest people are the one love beyond all faults, cry behind closed doors and fight battles that nobody knows about。)', '想研究外星人么？ 去爱大吧！！ 爱丁堡大学计划今年年底第一次提供寻找外星人的课程。。\"Introduction to Astrobiology and the Search for Extraterrestrial Life\" 将由学校的星际生物学教授Charles Cockell 讲授！']\n",
      "--------------------\n",
      "['【#越南乳瓜#】神奇的越南乳瓜，神似女性的乳房，颜色略呈粉色。乳瓜成熟果实含葡萄糖、果糖、蔗糖、胡萝卜素、维生素C、酒石酸、枸椽酸、苹果酸等。未成熟果实的汁液中含多量的乳瓜蛋白酶、脂肪酶。营养价值也很高。可是这样的瓜你敢吃吗。#我心中的乳神# \\u200b', '【阿婆厕所捡婴儿 被计生干部查获后活活摔死】15日下午，刘阿婆从乡财政所厕所粪便中将孩子捞起，简单清洗，剪脐带打针消毒。处理妥当正给孩子喂水时，武汉黄陂区蔡店乡计生办5人出现，夺走孩子掼在地上，用脚踢，放稻田里淹。引起当地群众的公愤。 @上海派對SHClubbing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据来源文件夹 -- 内含多个json文件\n",
    "non_rumor = './Chinese_Rumor_Dataset/CED_Dataset/non-rumor-repost'\n",
    "rumor = './Chinese_Rumor_Dataset/CED_Dataset/rumor-repost'\n",
    "original = './Chinese_Rumor_Dataset/CED_Dataset/original-microblog'\n",
    "\n",
    "non_rumor_data = []\n",
    "rumor_data = []\n",
    "\n",
    "# 遍历文件夹，读取文本数据\n",
    "print('开始读取数据')\n",
    "for file in tqdm(os.listdir(original)):\n",
    "    try:\n",
    "        data = json.load(open(os.path.join(original, file), 'rb'))['text']\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    is_rumor = (file in os.listdir(rumor))\n",
    "    if is_rumor:\n",
    "        rumor_data.append(data)\n",
    "    else:\n",
    "        non_rumor_data.append(data)\n",
    "\n",
    "print('结束, 有{}条谣言, 有{}条非谣言!'.format(len(rumor_data), len(non_rumor_data)))\n",
    "print(non_rumor_data[-2:])\n",
    "print('-'*20)\n",
    "print(rumor_data[-2:])\n",
    "\n",
    "\n",
    "# 把数据储存到指定地方 -- 统一到2个txt文件\n",
    "pth = './rumor_detection_data'\n",
    "if not os.path.exists(pth):\n",
    "    os.makedirs(pth)\n",
    "\n",
    "good_file = os.path.join(pth, 'non_rumor.txt')\n",
    "bad_file = os.path.join(pth, 'rumor.txt')\n",
    "\n",
    "# with open(good_file, 'w', encoding='utf-8') as f:\n",
    "#     f.write('\\n'.join(non_rumor_data))\n",
    "# with open(bad_file, 'w', encoding='utf-8') as f:\n",
    "#     f.write('\\n'.join(rumor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./rumor_detection_data/non_rumor.txt 包含 1849 行\n",
      "./rumor_detection_data/rumor.txt 包含 1538 行\n",
      "最长的句子里的未分词的词数:227\n"
     ]
    }
   ],
   "source": [
    "# 将文本中的标点符号过滤掉\n",
    "def filter_punc(sentence):\n",
    "    sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？?、~@#￥%……&*（）：:；“”】》《-【\\][]\", \"\",sentence.strip())\n",
    "    return sentence\n",
    "\n",
    "# 扫描所有的文本，分词、建立词典，分出是谣言还是非谣言，is_filter可以过滤是否筛选掉标点符号\n",
    "def Prepare_data(good_file, bad_file, is_filter = True):\n",
    "    max_len = 0\n",
    "    pos_sentences = [] #存储非谣言\n",
    "    neg_sentences = [] #存储谣言\n",
    "    with open(good_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                #过滤标点符号\n",
    "                line = filter_punc(line)\n",
    "                if not idx: # 只打印第一个例子看看\n",
    "                    print('分词前：', line)\n",
    "            if len(line) > 0:\n",
    "                pos_sentences.append(line)\n",
    "                max_len = max(max_len,len(line))\n",
    "    print('{0} 包含 {1} 行'.format(good_file, idx+1))\n",
    "\n",
    "    with open(bad_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line.strip())\n",
    "            if len(line) > 0:\n",
    "                neg_sentences.append(line)\n",
    "                max_len = max(max_len,len(line))\n",
    "    print('{0} 包含 {1} 行'.format(bad_file, idx+1))\n",
    "    print(f'最长的句子里的未分词的词数:{max_len}')\n",
    "\n",
    "    return pos_sentences, neg_sentences\n",
    "\n",
    "\n",
    "pos_sentences, neg_sentences =  Prepare_data(good_file, bad_file, is_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [] #标签\n",
    "sentences = [] #原始句子，未分词\n",
    "\n",
    "# 处理非谣言\n",
    "for sentence in pos_sentences:\n",
    "    labels.append(0) #正标签为0 表示非谣言数据\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# 处理谣言\n",
    "for sentence in neg_sentences:\n",
    "    labels.append(1) #负标签为1\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# 打乱所有的数据顺序，形成数据集\n",
    "# indices为所有数据下标的一个全排列\n",
    "indices = np.random.permutation(len(sentences))\n",
    "\n",
    "#对整个数据集进行划分，分为：训练集、验证集和测试集，这里是2:1:1\n",
    "test_size = len(sentences) // 4\n",
    "\n",
    "data = {\n",
    "    'labels': labels,# 标签\n",
    "    'sentences': sentences,# 句子\n",
    "}\n",
    "split = {\n",
    "    'train': indices[2*test_size:],\n",
    "    'vali': indices[:test_size],\n",
    "    'test': indices[test_size:2*test_size]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 非谣言有938条，谣言有757条\n",
      "vali 非谣言有478条，谣言有368条\n",
      "test 非谣言有433条，谣言有413条\n"
     ]
    }
   ],
   "source": [
    "# 测试一下划分情况\n",
    "while True:\n",
    "    tag = True\n",
    "    for key, indices in split.items():\n",
    "        count = [0, 0]\n",
    "        for idx in indices:\n",
    "            count[labels[idx]] += 1\n",
    "        # 如果类别过于不平衡，则重新随机化\n",
    "        cls_ratio = count[0]/count[1]\n",
    "        ratio_threshold = 1.5\n",
    "        if cls_ratio > ratio_threshold or cls_ratio < 1/ratio_threshold:\n",
    "            indices = np.random.permutation(len(bow))\n",
    "            tag = False\n",
    "            break\n",
    "        print(key, '非谣言有{}条，谣言有{}条'.format(count[0], count[1]))\n",
    "        if key==2:\n",
    "            tag = True\n",
    "    if tag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3060  297  466 ... 2818 2004  126]\n",
      "<pytorch_pretrained.tokenization.BertTokenizer object at 0x7f466aba2580>\n",
      "./Bert_Chinese_Text_Classification_Pytorch/cn_bert_wwm\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1695/1695 [00:00<00:00, 3365.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的分词后的词数:156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 846/846 [00:00<00:00, 3313.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的分词后的词数:171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 846/846 [00:00<00:00, 3386.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的分词后的词数:156\n",
      "Time usage: 0:00:01\n",
      "Epoch [1/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charon/hw/ML/rumor/./Bert_Chinese_Text_Classification_Pytorch/pytorch_pretrained/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:      0,  Train Loss:  0.73,  Train Acc: 43.75%,  Val Loss:  0.68,  Val Acc: 56.86%,  Time: 0:00:05 *\n",
      "Iter:    100,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:   0.4,  Val Acc: 82.74%,  Time: 0:00:36 *\n",
      "Epoch [2/50]\n",
      "Iter:    200,  Train Loss:  0.49,  Train Acc: 81.25%,  Val Loss:  0.43,  Val Acc: 82.27%,  Time: 0:01:07 \n",
      "Epoch [3/50]\n",
      "Iter:    300,  Train Loss: 0.074,  Train Acc: 100.00%,  Val Loss:  0.31,  Val Acc: 89.13%,  Time: 0:01:38 *\n",
      "Epoch [4/50]\n",
      "Iter:    400,  Train Loss: 0.042,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 88.42%,  Time: 0:02:08 \n",
      "Epoch [5/50]\n",
      "Iter:    500,  Train Loss: 0.0038,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 90.19%,  Time: 0:02:39 *\n",
      "Epoch [6/50]\n",
      "Iter:    600,  Train Loss: 0.035,  Train Acc: 100.00%,  Val Loss:  0.35,  Val Acc: 90.31%,  Time: 0:03:11 *\n",
      "Epoch [7/50]\n",
      "Iter:    700,  Train Loss: 0.0016,  Train Acc: 100.00%,  Val Loss:  0.54,  Val Acc: 88.30%,  Time: 0:03:41 \n",
      "Epoch [8/50]\n",
      "Iter:    800,  Train Loss: 0.00079,  Train Acc: 100.00%,  Val Loss:  0.42,  Val Acc: 90.66%,  Time: 0:04:12 *\n",
      "Epoch [9/50]\n",
      "Iter:    900,  Train Loss: 0.00094,  Train Acc: 100.00%,  Val Loss:  0.61,  Val Acc: 88.65%,  Time: 0:04:43 \n",
      "Epoch [10/50]\n",
      "Iter:   1000,  Train Loss: 0.00086,  Train Acc: 100.00%,  Val Loss:  0.47,  Val Acc: 89.83%,  Time: 0:05:14 \n",
      "Epoch [11/50]\n",
      "Iter:   1100,  Train Loss: 0.00049,  Train Acc: 100.00%,  Val Loss:  0.59,  Val Acc: 89.13%,  Time: 0:05:44 \n",
      "Epoch [12/50]\n",
      "Iter:   1200,  Train Loss: 0.00042,  Train Acc: 100.00%,  Val Loss:  0.51,  Val Acc: 90.43%,  Time: 0:06:15 \n",
      "Epoch [13/50]\n",
      "Iter:   1300,  Train Loss: 0.00033,  Train Acc: 100.00%,  Val Loss:  0.48,  Val Acc: 91.37%,  Time: 0:06:46 *\n",
      "Epoch [14/50]\n",
      "Iter:   1400,  Train Loss: 0.001,  Train Acc: 100.00%,  Val Loss:  0.51,  Val Acc: 90.90%,  Time: 0:07:17 \n",
      "Epoch [15/50]\n",
      "Iter:   1500,  Train Loss: 0.00024,  Train Acc: 100.00%,  Val Loss:   0.5,  Val Acc: 91.49%,  Time: 0:07:48 *\n",
      "Epoch [16/50]\n",
      "Iter:   1600,  Train Loss: 0.00029,  Train Acc: 100.00%,  Val Loss:  0.53,  Val Acc: 90.78%,  Time: 0:08:19 \n",
      "Epoch [17/50]\n",
      "Iter:   1700,  Train Loss: 0.00024,  Train Acc: 100.00%,  Val Loss:  0.53,  Val Acc: 90.90%,  Time: 0:08:50 \n",
      "Iter:   1800,  Train Loss: 0.00019,  Train Acc: 100.00%,  Val Loss:  0.54,  Val Acc: 91.02%,  Time: 0:09:21 \n",
      "Epoch [18/50]\n",
      "Iter:   1900,  Train Loss: 0.0077,  Train Acc: 100.00%,  Val Loss:  0.55,  Val Acc: 91.02%,  Time: 0:09:51 \n",
      "Epoch [19/50]\n",
      "Iter:   2000,  Train Loss: 0.00018,  Train Acc: 100.00%,  Val Loss:  0.56,  Val Acc: 90.90%,  Time: 0:10:22 \n",
      "Epoch [20/50]\n",
      "Iter:   2100,  Train Loss: 0.00014,  Train Acc: 100.00%,  Val Loss:  0.57,  Val Acc: 90.90%,  Time: 0:10:53 \n",
      "Epoch [21/50]\n",
      "Iter:   2200,  Train Loss: 0.00014,  Train Acc: 100.00%,  Val Loss:  0.59,  Val Acc: 90.54%,  Time: 0:11:24 \n",
      "Epoch [22/50]\n",
      "Iter:   2300,  Train Loss: 0.00016,  Train Acc: 100.00%,  Val Loss:  0.57,  Val Acc: 90.90%,  Time: 0:11:54 \n",
      "Epoch [23/50]\n",
      "Iter:   2400,  Train Loss: 0.00024,  Train Acc: 100.00%,  Val Loss:  0.58,  Val Acc: 91.02%,  Time: 0:12:25 \n",
      "Epoch [24/50]\n",
      "Iter:   2500,  Train Loss: 0.00014,  Train Acc: 100.00%,  Val Loss:  0.59,  Val Acc: 91.13%,  Time: 0:12:56 \n",
      "No optimization for a long time, auto-stopping...\n",
      "Test Loss:   0.5,  Test Acc: 91.37%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-rumor     0.8913    0.9469    0.9183       433\n",
      "       rumor     0.9404    0.8789    0.9086       413\n",
      "\n",
      "    accuracy                         0.9137       846\n",
      "   macro avg     0.9159    0.9129    0.9134       846\n",
      "weighted avg     0.9153    0.9137    0.9136       846\n",
      "\n",
      "Confusion Matrix...\n",
      "[[410  23]\n",
      " [ 50 363]]\n",
      "Time usage: 0:00:04\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "sys.path.append('./Bert_Chinese_Text_Classification_Pytorch/')\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bert_Chinese_Text_Classification_Pytorch.train_eval import train, init_network\n",
    "from importlib import import_module\n",
    "import argparse\n",
    "from Bert_Chinese_Text_Classification_Pytorch.utils import build_rumor_dataset, build_iterator, get_time_dif\n",
    "\n",
    "\n",
    "\n",
    "def run(data,split,bert_path='./Bert_Chinese_Text_Classification_Pytorch/bert_pretrain',freeze_bert=False):\n",
    "    split_indices = split\n",
    "    data = data\n",
    "\n",
    "    model_name = 'bert'\n",
    "    x = import_module('models.' + model_name)\n",
    "    config = x.Rumor_Config(split_indices)\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Loading data...\")\n",
    "    train_data, dev_data, test_data = build_rumor_dataset(data,config)\n",
    "    train_iter = build_iterator(train_data, config)\n",
    "    dev_iter = build_iterator(dev_data, config)\n",
    "    test_iter = build_iterator(test_data, config)\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # train\n",
    "    model = x.Model(config).to(config.device)\n",
    "    train(config, model, train_iter, dev_iter, test_iter,freeze_bert=freeze_bert)\n",
    "\n",
    "run(data,split,freeze_bert=False)\n",
    "# run(data,split,bert_path='./Bert_Chinese_Text_Classification_Pytorch/cn_bert_wwm',freeze_bert=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('qbw_base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "931916ec1e9018a72ab9e17be70c974d037672808d5f8fd77a6dc58db3f6bcd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
